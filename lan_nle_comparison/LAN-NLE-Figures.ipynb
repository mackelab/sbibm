{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "004c8e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import sbibm\n",
    "import torch\n",
    "\n",
    "# Plotting settings\n",
    "mpl.rcParams[\"axes.spines.top\"] = False\n",
    "mpl.rcParams[\"axes.spines.right\"] = False\n",
    "mpl.rcParams[\"axes.labelsize\"] = \"medium\"\n",
    "mpl.rcParams[\"legend.frameon\"] = False\n",
    "mpl.rcParams[\"legend.fontsize\"] = 14\n",
    "mpl.rcParams[\"font.size\"] = 20\n",
    "\n",
    "# Fix seed for figures.\n",
    "seed = 223174 #torch.randint(1000000, (1,))\n",
    "torch.manual_seed(seed)\n",
    "print(f\"seed {seed}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce1d2a31",
   "metadata": {},
   "source": [
    "## Load DDM task from `sbibm`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5279e900",
   "metadata": {},
   "outputs": [],
   "source": [
    "task = sbibm.get_task(\"ddm\")\n",
    "prior = task.get_prior_dist()\n",
    "simulator = task.get_simulator(seed=seed) # Passing the seed to Julia."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4229e528",
   "metadata": {},
   "source": [
    "### Load pretrained LANs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "868009d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# network trained on KDE likelihood for 4-param ddm\n",
    "lan_kde_path = \"../sbibm/algorithms/lan/lan_pretrained/model_final_ddm.h5\"\n",
    "lan_ana_path = \"../sbibm/algorithms/lan/lan_pretrained/model_final_ddm_analytic.h5\"\n",
    "lan_kde = keras.models.load_model(lan_kde_path, compile=False)\n",
    "lan_ana = keras.models.load_model(lan_ana_path, compile=False)\n",
    "\n",
    "def lan_likelihood(theta, data, net, ll_lower_bound):\n",
    "    \"\"\"Return log likelihood summed over all trials in data, \n",
    "        given a batch of parameters theta.\n",
    "    \n",
    "    Args\n",
    "        theta: batch of parameters\n",
    "        data: batch of iid reaction times and choices encoded\n",
    "            as negative and positive reaction times. \n",
    "        net: lan keras model\n",
    "        ll_lower_bound: lower bound of single trial log likelihood.\n",
    "        \n",
    "    Returns\n",
    "        llsum: batch of log likelihoods over trials. \n",
    "    \"\"\"\n",
    "    \n",
    "    # Convert to positive rts.\n",
    "    rts = abs(data)\n",
    "    num_trials = rts.numel()\n",
    "    num_parameters = theta.shape[0]\n",
    "    assert rts.shape == torch.Size([num_trials, 1])\n",
    "    theta = torch.tensor(theta, dtype=torch.float32)\n",
    "    # Convert DDM boundary seperation to symmetric boundary size.\n",
    "    theta[:, 1] *= 0.5\n",
    "\n",
    "    # Code down -1 up +1.\n",
    "    cs = torch.ones_like(rts)\n",
    "    cs[data < 0] *= -1\n",
    "\n",
    "    # Repeat theta trial times\n",
    "    theta_repeated = theta.repeat(num_trials, 1)\n",
    "    # repeat trial data theta times.\n",
    "    rts_repeated = torch.repeat_interleave(rts, num_parameters, dim=0)\n",
    "    cs_repeated = torch.repeat_interleave(cs, num_parameters, dim=0)\n",
    "\n",
    "    # stack everything for the LAN net.\n",
    "    theta_x_stack = torch.cat((theta_repeated, rts_repeated, cs_repeated), dim=1)\n",
    "    ll_each_trial = torch.tensor(\n",
    "        net.predict_on_batch(theta_x_stack.numpy()),\n",
    "        dtype=torch.float32,\n",
    "    ).reshape(num_trials, num_parameters)\n",
    "\n",
    "    # Lower bound on each trial log likelihood.\n",
    "    # Sum across trials.\n",
    "    llsum = torch.where(\n",
    "        torch.logical_and(\n",
    "            rts.repeat(1, num_parameters) > theta[:, -1], \n",
    "            ll_each_trial > ll_lower_bound,\n",
    "        ),\n",
    "        ll_each_trial,\n",
    "        ll_lower_bound * torch.ones_like(ll_each_trial),\n",
    "    ).sum(0)\n",
    "    \n",
    "    return llsum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b76dd7ea",
   "metadata": {},
   "source": [
    "## Likelihood comparison for single example\n",
    "\n",
    "##### Sample example parameter from prior\n",
    "\n",
    "For creating a figure showing the likelihood over the entire data space given a fixed parameter combination we sample a single parameter combination from the prior and evaluate the synthetic likelihoods for a large range of reaction times and both choices, while holding the parameters fixed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de62e5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample one parameter from the prior for a likelihood example.\n",
    "theta_o = prior.sample((1,))\n",
    "l_lower_bound = 1e-7\n",
    "theta_o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e92c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pretrained NLE model\n",
    "with open(\"../sbibm/algorithms/lan/nle_pretrained/mm_315_2.p\", \"rb\") as fh: \n",
    "    nle = pickle.load(fh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a28671",
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct rts and choices for nle in [-test_tmax, test_tmax]\n",
    "\n",
    "# RT range\n",
    "test_tmax = 5\n",
    "# Number of test points\n",
    "ntest = 1000\n",
    "rs = torch.cat((torch.linspace(test_tmax, 1e-7, ntest//2), \n",
    "           torch.linspace(1e-7, test_tmax, ntest//2))).reshape(-1, 1)\n",
    "cs = torch.cat((torch.zeros(ntest//2), \n",
    "           torch.ones(ntest//2))).reshape(-1, 1)\n",
    "\n",
    "test_rts = torch.linspace(-test_tmax, test_tmax, ntest)\n",
    "\n",
    "# get NLE synthetic likelihood for each data point with fixed theta.\n",
    "lps_nle = torch.tensor([nle.log_prob(r.reshape(-1, 1), c.reshape(-1, 1), theta_o) for r, c in zip(rs, cs)])\n",
    "\n",
    "# from analytical likelihood\n",
    "lps_true = torch.tensor([task.get_log_likelihood(theta_o, d.reshape(-1, 1), l_lower_bound=l_lower_bound) \n",
    "                     for d in test_rts])\n",
    "\n",
    "# and from both LANs.\n",
    "lps_lanana = torch.tensor([lan_likelihood(theta_o, \n",
    "                                   d.reshape(-1, 1), \n",
    "                                   net=lan_ana, \n",
    "                                   ll_lower_bound=np.log(l_lower_bound)) \n",
    "                    for d in test_rts])\n",
    "lps_lankde = torch.tensor([lan_likelihood(theta_o, \n",
    "                                   d.reshape(-1, 1), \n",
    "                                   net=lan_kde, \n",
    "                                   ll_lower_bound=np.log(l_lower_bound)) \n",
    "                    for d in test_rts])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d96868e",
   "metadata": {},
   "source": [
    "## Systematic Likelihood comparison\n",
    "\n",
    "Next we do a systematic comparison based on the Huber loss (the LAN training loss) and the mean squared error (MSE) between analytical and synthetic log likelihoods of LAN and NLE.\n",
    "\n",
    "To mimick the inference setup we sample an observation from the simulator via parameters from the prior and obtain the likelihood of this observation given a large batch of parameters, e.g., `1000`, sampled from the prior. The Huber loss and MSE is then calculated across this batch, giving a single number. This procedure we repeat for many observation, e.g., `100` and then show boxplots over the resulting 100 numbers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b0567e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define losses.\n",
    "def huberloss(y, yhat):\n",
    "    diff = abs(y-yhat)\n",
    "    \n",
    "    err = np.zeros(y.numel())\n",
    "    err[diff <= 1.0] = 0.5 * diff[diff <= 1.0]**2\n",
    "    err[diff > 1.0] = 0.5 + diff[diff > 1.0]\n",
    "    return err.mean()\n",
    "\n",
    "def mse(y, yhat):\n",
    "    return torch.mean((y - yhat)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c62f219",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mimick the MCMC setting: xo is fixed, thetas are scored with loss\n",
    "num_observations = 100\n",
    "num_thetas_per_observation = 1000\n",
    "\n",
    "# first sample observations xo\n",
    "xos = simulator(prior.sample((num_observations,)))\n",
    "\n",
    "labels = [\n",
    "#     \"LAN-ANA\", \n",
    "    \"LAN\", \n",
    "    \"NLE\",\n",
    "]\n",
    "errors = []\n",
    "for xoi in xos:\n",
    "    \n",
    "    # Sample test thetas from prior.\n",
    "    test_thetas = prior.sample((num_thetas_per_observation,))\n",
    "    xoi = xoi.reshape(-1, 1)\n",
    "    # Extract positive RTs and choices for mixed model.\n",
    "    rsi = abs(xoi)\n",
    "    csi = torch.ones_like(rsi)\n",
    "    csi[xoi < 0] = 0\n",
    "\n",
    "    # Evaluate\n",
    "    lps_nle_i = nle.log_prob(rsi, csi, test_thetas).squeeze()    \n",
    "    lps_true_i = task.get_log_likelihood(test_thetas, data=xoi.reshape(1, -1), l_lower_bound=l_lower_bound)\n",
    "    lps_lanana_i = lan_likelihood(test_thetas, xoi, lan_ana, np.log(l_lower_bound))\n",
    "    lps_lankde_i = lan_likelihood(test_thetas, xoi, lan_kde, np.log(l_lower_bound))\n",
    "    \n",
    "\n",
    "    # Score\n",
    "    errors.append([\n",
    "            [\n",
    "#                 huberloss(lps_lanana_i, lps_true_i),\n",
    "                huberloss(lps_lankde_i, lps_true_i),\n",
    "                huberloss(lps_nle_i, lps_true_i),\n",
    "            ],\n",
    "            [\n",
    "#                 huberloss(lps_lanana_i.exp(), lps_true_i.exp()),\n",
    "                huberloss(lps_lankde_i.exp(), lps_true_i.exp()),\n",
    "                huberloss(lps_nle_i.exp(), lps_true_i.exp()),\n",
    "            ],            \n",
    "            [\n",
    "#                 mse(lps_lanana_i, lps_true_i),\n",
    "                mse(lps_lankde_i, lps_true_i),\n",
    "                mse(lps_nle_i, lps_true_i),\n",
    "            ], \n",
    "            [\n",
    "#                 mse(lps_lanana_i.exp(), lps_true_i.exp()),\n",
    "                mse(lps_lankde_i.exp(), lps_true_i.exp()),\n",
    "                mse(lps_nle_i.exp(), lps_true_i.exp()),\n",
    "            ],         \n",
    "        ])\n",
    "errors = np.array(errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b6621e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract separate dataframes for huber and mse.\n",
    "dfhuber_log = pd.DataFrame(data=np.array(errors)[:, 0, :], columns=labels)\n",
    "dfhuber = pd.DataFrame(data=np.array(errors)[:, 1, :], columns=labels)\n",
    "dfmse_log = pd.DataFrame(data=np.array(errors)[:, 2, :], columns=labels)\n",
    "dfmse = pd.DataFrame(data=np.array(errors)[:, 3, :], columns=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cae9bc7",
   "metadata": {},
   "source": [
    "## Estimate evaluation times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e39964e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "# Vary size of theta (number of MCMC chains in parallel)\n",
    "num_chains = [10]\n",
    "# Vary size of data (number of trials)\n",
    "num_trials = [100]\n",
    "reps = 100\n",
    "\n",
    "lan_rts = np.zeros((len(num_chains), len(num_trials), reps))\n",
    "nle_rts = np.zeros_like(lan_rts)\n",
    "\n",
    "thetas = prior.sample((num_chains[-1],))\n",
    "xs = simulator(prior.sample((1,)).repeat(num_trials[-1], 1))\n",
    "\n",
    "for ii in range(reps):\n",
    "    for jj, nc in enumerate(num_chains): \n",
    "        for kk, nt in enumerate(num_trials):\n",
    "            # LAN timing\n",
    "            tic = time.time()\n",
    "            lan_likelihood(thetas[:nc,], xs[:nt,], net=lan_kde, ll_lower_bound=np.log(l_lower_bound))\n",
    "            lan_rts[jj, kk, ii] = time.time() - tic\n",
    "\n",
    "            # NLE timing\n",
    "            rts = abs(xs[:nt])\n",
    "            cs = torch.ones_like(rts)\n",
    "            cs[xs[:nt] < 0] = 0\n",
    "            tic = time.time()\n",
    "            nle.log_prob(rts, cs, thetas[:nc])\n",
    "            nle_rts[jj, kk, ii] = time.time() - tic\n",
    "\n",
    "# convert ot ms\n",
    "lan_rts *= 1000\n",
    "nle_rts *= 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e01f4e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "nle_rts.mean(), nle_rts.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f694b2f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "lan_rts.mean(), lan_rts.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d27d31a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rt = pd.DataFrame(data={\"LAN-KDE\":lan_rts.reshape(-1), \"NLE\": nle_rts.reshape(-1)}, index=range(reps))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d94251",
   "metadata": {},
   "source": [
    "## Results figure 1\n",
    "\n",
    "- likelihood examples\n",
    "\n",
    "- likelihood accuracy\n",
    "\n",
    "- number of simulations\n",
    "\n",
    "- evaluation time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa2d8483",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 3, sharex=False, figsize=(18, 8),  \n",
    "                       gridspec_kw=dict(wspace=0.25, hspace=0.2, width_ratios=[0.6, .2, .2]))\n",
    "\n",
    "mpl.rcParams[\"legend.fontsize\"] = 12\n",
    "mpl.rcParams[\"font.size\"] = 15\n",
    "colors = [\n",
    "#     \"C1\", \n",
    "    \"C2\", \n",
    "    \"C3\"\n",
    "]\n",
    "grid = True\n",
    "showfliers = True\n",
    "\n",
    "labels = [\"Analytical\", \n",
    "#           \"LAN-ANA\", \n",
    "          \"LAN\", \n",
    "          \"NLE\", \n",
    "         ]\n",
    "\n",
    "plt.sca(ax[0, 0])\n",
    "plt.plot(test_rts, lps_true.exp(), label=\"Analytical L\");\n",
    "# plt.plot(test_rts, lps_lanana.exp(), label=\"LAN-ANA\");\n",
    "plt.plot(test_rts, lps_lankde.exp(), label=\"LAN-KDE\", ls=\"-\", c=\"C2\");\n",
    "plt.plot(test_rts, lps_nle.exp(), label=\"NLE\", ls=\"-\", c=\"C3\");\n",
    "plt.ylabel(r\"$L(x | \\theta)$\");\n",
    "plt.legend(labels)\n",
    "plt.xticks([-4, -2, 0, 2, 4], [])\n",
    "# plt.yticks([0, .4, .8, 1.2], [0, .4, .8, 1.2])\n",
    "plt.axvline(0, color=\"k\", lw=1)\n",
    "y = max(lps_true.exp())+.1\n",
    "plt.arrow(0, y, 0.3, 0., width=0.03, color=\"k\", alpha=0.5)\n",
    "plt.text(0.25, 1.05 * y, s=\"up\")\n",
    "plt.arrow(0, y, -0.3, 0., width=0.03, color=\"k\", alpha=0.5)\n",
    "plt.text(-.94, 1.05*y, s=\"down\")\n",
    "# plt.suptitle(fr\"v={theta_o[0, 0]:.2f}, a={theta_o[0, 1]:.2f}, w={theta_o[0, 2]:.2f}, $\\tau$={theta_o[0, 3]:.2f}\");\n",
    "\n",
    "plt.sca(ax[1, 0])\n",
    "plt.plot(test_rts, lps_true)\n",
    "# plt.plot(test_rts, lps_lanana)\n",
    "plt.plot(test_rts, lps_lankde, ls=\"-\", c=\"C2\")\n",
    "plt.plot(test_rts, lps_nle, ls=\"-\", c=\"C3\")\n",
    "# plt.legend(labels)\n",
    "plt.xlabel(\"$x$: reaction time [s]\")\n",
    "plt.ylabel(r\"$\\log L(x | \\theta)$\");\n",
    "plt.xticks([-4, -2, 0, 2, 4], [4, 2, 0, 2, 4])\n",
    "plt.axvline(0, color=\"k\", lw=1)\n",
    "\n",
    "\n",
    "\n",
    "plt.sca(ax[0, 1])\n",
    "box_widths = [0.3] * len(colors)\n",
    "bdict = dfhuber_log.boxplot(ax=ax[0, 1], patch_artist=True, return_type=\"dict\", \n",
    "                        medianprops={\"color\": \"k\"}, grid=grid, \n",
    "                           notch=True, \n",
    "                           widths=box_widths, \n",
    "                           showfliers=showfliers,\n",
    "                           )\n",
    "plt.ylabel(\"Huber loss\");\n",
    "for i,box in enumerate(bdict['boxes']):\n",
    "    box.set_color(colors[i])\n",
    "plt.yticks(np.linspace(0, 0.4, 3));\n",
    "plt.ylim(0, .4)\n",
    "\n",
    "plt.sca(ax[0, 2])\n",
    "bdict = dfmse_log.boxplot(ax=ax[0, 2], patch_artist=True, return_type=\"dict\", \n",
    "                          medianprops={\"color\": \"k\"}, grid=grid, \n",
    "                          notch=True, \n",
    "                          widths=box_widths,\n",
    "                          showfliers=showfliers,\n",
    "                         )\n",
    "plt.ylabel(r\"MSE\");\n",
    "for i,box in enumerate(bdict['boxes']):\n",
    "    box.set_color(colors[i])\n",
    "plt.yticks(np.linspace(0, 2.5, 3));\n",
    "\n",
    "\n",
    "ddd = pd.DataFrame({'method': [\"LAN\", \"NLE\"], 'training budget': [15e9, 1e5]})\n",
    "plt.sca(ax[1, 2])\n",
    "ddd.plot.bar(x=\"method\", y=\"training budget\", color=[\"C2\", \"C3\"], ax=ax[1, 2], \n",
    "             rot=0, width=box_widths[0])\n",
    "plt.xlabel('')\n",
    "plt.ylabel(\"training budget\")\n",
    "plt.legend(\"\")\n",
    "plt.yscale(\"log\")\n",
    "plt.yticks(np.logspace(5, 10, 2), [r\"$10^5$\", \n",
    "#                                    r\"$10^6$\",r\"$10^7$\",r\"$10^8$\",r\"$10^9$\", \n",
    "                                   r\"$10^{10}$\"])\n",
    "# plt.grid()\n",
    "\n",
    "ddd = pd.DataFrame({'method': [\"LAN\", \"NLE\"], 'rt': [lan_rts.mean(), nle_rts.mean()]})\n",
    "plt.sca(ax[1, 1])\n",
    "ddd.plot.bar(x=\"method\", y=\"rt\", color=[\"C2\", \"C3\"], ax=ax[1, 1], \n",
    "             rot=0, width=box_widths[0], \n",
    "            yerr=[lan_rts.std()/np.sqrt(reps), nle_rts.std()/np.sqrt(reps)])\n",
    "plt.xlabel('')\n",
    "plt.ylabel(\"evaluation time [ms]\")\n",
    "plt.legend(\"\")\n",
    "plt.yticks(np.linspace(0, 6, 4), np.linspace(0, 6, 4))\n",
    "\n",
    "weight = \"bold\"\n",
    "fontsize = 20\n",
    "y1 = 0.9\n",
    "x1 = 0.075\n",
    "fig.text(x1, y1, \"A\", fontsize=fontsize, fontweight=weight)\n",
    "fig.text(x1 + 0.46, y1, \"B\", fontsize=fontsize, fontweight=weight)\n",
    "fig.text(x1 + 0.64, y1, \"C\", fontsize=fontsize, fontweight=weight)\n",
    "fig.text(x1 + .46, y1 - 0.42, \"D\", fontsize=fontsize, fontweight=weight)\n",
    "fig.text(x1 + 0.64, y1 - 0.42, \"E\", fontsize=fontsize, fontweight=weight);\n",
    "plt.tight_layout()\n",
    "fig.savefig(\"LAN-NLE-likelihood-comparison.png\", dpi=300, bbox_inches=\"tight\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0b25758",
   "metadata": {},
   "source": [
    "## Figure 2: posterior space\n",
    "\n",
    "- posterior example\n",
    "- posterior metrics, timings should be according to likelihood evaluation timings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b76f66f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../../results/benchmarking_sbi/\")\n",
    "from utils import compile_df\n",
    "\n",
    "# from sbibm.utils import compile_df\n",
    "from sbibm.utils.io import get_tensor_from_csv, get_ndarray_from_csv\n",
    "import pandas as pd\n",
    "from sbi.analysis import pairplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc34f5b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([\n",
    "    compile_df(\"../../results/benchmarking_sbi/multirun/2021-10-01/09-59-40/\"),  # LAN 1-10\n",
    "    compile_df(\"../../results/benchmarking_sbi/multirun/2021-10-01/18-35-25/\"),  # NLE 1-10\n",
    "    compile_df(\"../../results/benchmarking_sbi/multirun/2021-09-29/21-43-22/\"),  # LAN 11-20\n",
    "    compile_df(\"../../results/benchmarking_sbi/multirun/2021-10-06/07-58-18/\"),  # NLE 11-20\n",
    "    compile_df(\"../../results/benchmarking_sbi/multirun/2021-09-29/22-30-49/\"),  # LAN 21-30\n",
    "    compile_df(\"../../results/benchmarking_sbi/multirun/2021-09-30/09-16-33/\"),  # NLE 21-30\n",
    "    compile_df(\"../../results/benchmarking_sbi/multirun/2021-09-30/09-49-49/\"),  # NLE 31-40\n",
    "    compile_df(\"../../results/benchmarking_sbi/multirun/2021-09-30/11-48-19/\"),  # LaN 31-40\n",
    "    compile_df(\"../../results/benchmarking_sbi/multirun/2021-10-05/11-42-01/\"),  # NLE 41-50\n",
    "    compile_df(\"../../results/benchmarking_sbi/multirun/2021-10-05/12-35-48/\"),  # LaN 41-50\n",
    "    compile_df(\"../../results/benchmarking_sbi/multirun/2021-10-05/14-59-22/\"),  # NLE 51-60\n",
    "    compile_df(\"../../results/benchmarking_sbi/multirun/2021-10-05/15-25-27/\"),  # NLE 61-70\n",
    "    compile_df(\"../../results/benchmarking_sbi/multirun/2021-10-05/16-40-14/\"),  # NLE 71-80\n",
    "    compile_df(\"../../results/benchmarking_sbi/multirun/2021-10-05/17-05-47/\"),  # NLE 81-90\n",
    "    compile_df(\"../../results/benchmarking_sbi/multirun/2021-10-05/17-31-34/\"),  # NLE 91-100\n",
    "    compile_df(\"../../results/benchmarking_sbi/multirun/2021-10-05/17-57-20/\"),  # LAN 51-60\n",
    "    compile_df(\"../../results/benchmarking_sbi/multirun/2021-10-05/18-43-08/\"),  # LAN 61-70\n",
    "    compile_df(\"../../results/benchmarking_sbi/multirun/2021-10-05/19-29-10/\"),  # LAN 71-80\n",
    "    compile_df(\"../../results/benchmarking_sbi/multirun/2021-10-05/20-14-40/\"),  # LAN 81-90\n",
    "    compile_df(\"../../results/benchmarking_sbi/multirun/2021-10-05/20-59-55/\"),  # LAN 91-100\n",
    "]\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef9c1fc3",
   "metadata": {},
   "source": [
    "#### Load posterior samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "795b3288",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = 24\n",
    "labels = [\"LAN-KDE\", \"NLE\"]\n",
    "cols = [\"MEANERR\", \"VARERR\", \"C2ST\", \"RT\"]\n",
    "paths = []\n",
    "\n",
    "# df = df100\n",
    "for alg in labels:\n",
    "    idx = df[df.num_observation==obs].algorithm == alg\n",
    "    p = df[df.num_observation==obs].loc[idx, \"path\"].values\n",
    "    [paths.append(pi) for pi in p]\n",
    "\n",
    "ss = [\n",
    "    sbibm.get_task(\"ddm\").get_reference_posterior_samples(obs)\n",
    "     ] + [get_ndarray_from_csv(path+\"/posterior_samples.csv.bz2\") for path in paths]\n",
    "\n",
    "print(sbibm.get_task(\"ddm\").get_true_parameters(obs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ce203a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(18, 8))\n",
    "outer_grid = fig.add_gridspec(1, 2, wspace=0.15, hspace=0, width_ratios=[.6, .4])\n",
    "# Plotting settings\n",
    "mpl.rcParams[\"font.size\"] = 16\n",
    "notch = True\n",
    "\n",
    "# posterior samples\n",
    "num_plots = 4\n",
    "inner_grid = outer_grid[0, 0].subgridspec(num_plots, num_plots, wspace=0.09, hspace=0)\n",
    "ax1 = inner_grid.subplots()  # Create all subplots for the inner grid.\n",
    "pairplot(ss, \n",
    "         points=sbibm.get_task(\"ddm\").get_true_parameters(obs), \n",
    "         limits=[[-2, 2], [0.5, 2.0], [.3, .7], [.2, 1.]], \n",
    "         samples_colors=[\"C0\", \"C2\", \"C3\"], \n",
    "         diag=\"kde\",\n",
    "         upper=\"contour\",\n",
    "         kde_offdiag=dict(bw_method=\"scott\", bins=30),\n",
    "         contour_upper=dict(levels=[0.1], percentile=False),\n",
    "         points_offdiag=dict(marker=\"+\", markersize=10), \n",
    "         points_colors=[\"k\"], \n",
    "         fig=fig, \n",
    "         axes=ax1,\n",
    "         labels=[r\"$v$\", r\"$a$\", r\"$w$\", r\"$\\tau$\"],\n",
    "        );\n",
    "plt.sca(ax1[0, 0])\n",
    "plt.legend([\"Analytical\", \"LAN\", \"NLE\", r\"Ground truth $\\theta$\"], \n",
    "           bbox_to_anchor=(-.1, -2.2), \n",
    "           loc=2, \n",
    "          fontsize=16)\n",
    "\n",
    "\n",
    "# posterior metrics\n",
    "inner_grid = outer_grid[0, 1].subgridspec(2, 2, wspace=.7, hspace=.3, )\n",
    "ax2 = inner_grid.subplots()  # Create all subplots for the inner grid.\n",
    "\n",
    "bdict = df.boxplot(ax=ax2, column=cols, by=[\"algorithm\"], rot=0, \n",
    "                grid=True, \n",
    "                fontsize=14.0,\n",
    "                patch_artist=True,\n",
    "                widths=box_widths,\n",
    "                return_type=\"both\", medianprops={\"color\": \"k\"}, \n",
    "                notch=notch, \n",
    "                showfliers=showfliers,);\n",
    "\n",
    "colors = [\"C2\", \"C3\"]\n",
    "\n",
    "ticks = [\n",
    "    np.linspace(0, 2., 3), \n",
    "    np.linspace(0.0, .005, 3), \n",
    "    np.linspace(0.5, 0.9, 3),\n",
    "    np.linspace(20, 60, 3),\n",
    "]\n",
    "\n",
    "for a, t in zip(ax2.reshape(-1), ticks):\n",
    "    a.set_yticks(t)\n",
    "    a.set_ylim(t[0], t[-1])\n",
    "\n",
    "\n",
    "for row_key, (axi,row) in bdict.iteritems():\n",
    "    for i,box in enumerate(row['boxes']):\n",
    "        box.set_color(colors[i])\n",
    "        \n",
    "col_labels = [\"mean error\", \"variance error\", \"C2ST\", \"time [min]\"]\n",
    "\n",
    "for i, a in enumerate(ax2.reshape(-1)):\n",
    "    a.set_ylabel(col_labels[i])\n",
    "    a.set_title(\"\")\n",
    "    a.set_ylabel(col_labels[i])\n",
    "    a.set_xlabel(\"\")\n",
    "    a.set_xticklabels([\"LAN\", \"NLE\"], fontsize=mpl.rcParams[\"font.size\"])\n",
    "plt.suptitle(\"\")\n",
    "\n",
    "weight = \"bold\"\n",
    "fontsize = 20\n",
    "y1 = 0.9\n",
    "x1 = 0.09\n",
    "fig.text(x1, y1, \"A\", fontsize=fontsize, fontweight=weight)\n",
    "fig.text(x1 + 0.46, y1, \"B\", fontsize=fontsize, fontweight=weight)\n",
    "fig.text(x1 + 0.64, y1, \"C\", fontsize=fontsize, fontweight=weight)\n",
    "fig.text(x1 + 0.46, y1 - 0.42, \"D\", fontsize=fontsize, fontweight=weight)\n",
    "fig.text(x1 + 0.64, y1 - 0.42, \"E\", fontsize=fontsize, fontweight=weight)\n",
    "\n",
    "plt.tight_layout();\n",
    "fig.savefig(\"LAN-NLE-posterior-comparison.png\", dpi=300, bbox_inches=\"tight\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6189efc",
   "metadata": {},
   "source": [
    "## Visualize Likelihood for 10 Benchmark observations\n",
    "We can also visualize the single trial likelihoods for the 10 random observations used in the benchmark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a11d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "ll_true = []\n",
    "ll_lan = []\n",
    "ll_nle = []\n",
    "\n",
    "for ii in range(1, 11):\n",
    "    \n",
    "    xo = task.get_observation(ii).reshape(-1, 1)\n",
    "    test_thetas = prior.sample((1000,))\n",
    "\n",
    "    # Extract positive RTs and choices for mixed model.\n",
    "    rs = abs(xo)\n",
    "    cs = torch.ones_like(rs)\n",
    "    cs[xo < 0] = 0\n",
    "  \n",
    "    ll_true.append(task.get_log_likelihood(test_thetas, data=xo.reshape(1, -1), l_lower_bound=l_lower_bound))\n",
    "\n",
    "    ll_lan.append(lan_likelihood(test_thetas, \n",
    "                                xo, \n",
    "                                net=lan_kde, \n",
    "                                ll_lower_bound=np.log(l_lower_bound)))\n",
    "    \n",
    "    ll_nle.append(nle.log_prob(rs, cs, test_thetas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e63d3fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(3, 3, figsize=(18, 12), sharey=True, sharex=True)\n",
    "mpl.rcParams[\"axes.spines.right\"] = False\n",
    "mpl.rcParams[\"axes.spines.top\"] = False\n",
    "s = 7\n",
    "idx = 0\n",
    "alpha = 1.0\n",
    "for ii in range(0, 9):\n",
    "    if ii%3 == 0:\n",
    "        plt.sca(ax[idx, 0])\n",
    "    elif ii%3 == 1:\n",
    "        plt.sca(ax[idx, 1])\n",
    "    elif ii%3 == 2:\n",
    "        plt.sca(ax[idx, 2])\n",
    "        idx += 1\n",
    "\n",
    "    plt.title(f\"Observation {ii+1}\")\n",
    "    \n",
    "    plt.scatter(ll_true[ii], ll_lan[ii], alpha=alpha, color=\"C2\", s=s)\n",
    "    plt.scatter(ll_true[ii], ll_nle[ii], alpha=alpha, color=\"C3\", s=s)\n",
    "    plt.plot(ll_true[ii], ll_true[ii], \"k\")\n",
    "    if not ii:\n",
    "        plt.legend([\"Identity\", \"LAN\", \"NLE\", \n",
    "                   ], frameon=False, fontsize=12)\n",
    "    if ii in [0, 3, 6]:\n",
    "        plt.ylabel(\"synthetic log L\")\n",
    "    if ii > 5: \n",
    "        plt.xlabel(\"analytic log L\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d650e863",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
